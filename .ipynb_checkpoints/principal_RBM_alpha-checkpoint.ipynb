{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44fa49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import numpy.random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fd0ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scio.loadmat('binaryalphadigs.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9219c59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'dat', 'numclass', 'classlabels', 'classcounts'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5326b8c2",
   "metadata": {},
   "source": [
    "Each character is a 20x16 {0,1}-valued array\n",
    "\n",
    "__In order to have a {0,1}-valued vector, we need to flatten it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4089a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = data['dat']\n",
    "datasize = mat.shape\n",
    "sample_size = mat[0,0].shape\n",
    "new_format_size = (datasize[0],datasize[1],sample_size[0]*sample_size[1])\n",
    "# the new format is a flattened version of the raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cff5933c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(new_format_size)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char,samp \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m36\u001b[39m),\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m39\u001b[39m)) :\n\u001b[1;32m      3\u001b[0m         dataset[char,samp] \u001b[38;5;241m=\u001b[39m mat[char,samp]\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "dataset = np.zeros(new_format_size)\n",
    "for char in range(datasize[0]) :\n",
    "    for samp in range(datasize[1])\n",
    "        dataset[char,samp] = mat[char,samp].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f9461609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 39, 320)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1276e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire_alpha_digits(character_index_to_learn):\n",
    "    mats = data['dat']\n",
    "    return mats[character_index_to_learn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26273337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lire_alpha_digits(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57593c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, input_bias, output_bias ,weights):\n",
    "        self.a = input_bias\n",
    "        self.b = output_bias\n",
    "        self.W = weights\n",
    "    \n",
    "\n",
    "def init_RBM(i,o): # i and o are input and ouptut sizes\n",
    "    layer = RBM(np.zeros(i),np.zeros(o),rd.normal(loc = 0.0, scale = 0.1, size =(i,o) ))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf72845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entree_sortie_RBM(rbm, input_data):\n",
    "    return sigmoid(rbm.W.dot(input_data) + rbm.a)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b56857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortie_entree_RBM(rbm, output_data):\n",
    "    return sigmoid(output_data.dot(rbm.W.T) + rbm.b.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8231fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM(rbm, epochs, learning_rate, batch_size):\n",
    "    \n",
    "    for t in epoch :\n",
    "        print(\"Epoch : \", t, \"/\", epoch)\n",
    "        \n",
    "        for batch in data[::batch_size] :\n",
    "            h = entree_sortie(rbm, batch)\n",
    "            pos_grad = batch.dot(h)\n",
    "            \n",
    "            #Contrastive Divergence -1\n",
    "            batch_prime = sortie_entree(rbm, h)\n",
    "            h_prime = entree_sortie(rbm, batch_prime)\n",
    "            \n",
    "            neg_grad = batch_prime.dot(h_prime)\n",
    "            rbm.W -= learning_rate*(pos_grad-neg_grad)\n",
    "            rbm.a -= learning_rate*(batch - batch_prime)\n",
    "            rbm.b -= learning_rate*(h - h_prime)\n",
    "            \n",
    "            err = np.linalg.norm(batch - batch_prime, 2)\n",
    "            print(err)\n",
    "            \n",
    "    return rbm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e857ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1192963298.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def generer_image_RBM(rbm, num_of_it, num_of_img):\n",
    "    n = data['numclass'] # n is the population per class in the dataset\n",
    "    k = n/num_of_img # we generate the same amount of images per class\n",
    "    # let's generate k images per class :\n",
    "    for _class in data['classlabels'] :\n",
    "        for i in range(k):\n",
    "            # gibbs sampling....\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec07f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
